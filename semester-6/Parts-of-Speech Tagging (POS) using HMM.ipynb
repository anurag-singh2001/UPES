{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'IN']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'In\\tIN\\n'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# create vocab from the tarining data and save it in the file (hmm_vocab.txt)\n",
    "from collections import Counter\n",
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:5])\n",
    "words = [line.split('\\t')[0] for line in lines]\n",
    "freq= Counter(words)\n",
    "vocab= [key for key, value in freq.items() if (value > 2 and key!='\\n')]\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('In', 1740)\n",
      "('an', 3143)\n",
      "('Oct.', 318)\n",
      "('19', 100)\n",
      "('review', 58)\n",
      "('of', 22929)\n",
      "('``', 6967)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(freq.items()):\n",
    "    print(item)\n",
    "    if i>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$', '<', '>', '?', '.', '_', '*', ',', '#', '!', '/', '(', ':', '\\\\', '+', '^', '=', '~', ')', '|', '}', '[', '-', '{', '%', ';', \"'\", '&', '`', '\"', '@', ']'}\n"
     ]
    }
   ],
   "source": [
    "print(set(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "\n",
    "    punct = set(string.punctuation)\n",
    "    \n",
    "    # Suffixes\n",
    "    noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "    verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "    adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "    adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return \"--unk_digit--\"\n",
    "    \n",
    "    elif any(char in punct for char in word):\n",
    "        return \"--unk_punct--\"\n",
    "    \n",
    "    elif any(char.isupper() for char in word):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "    \n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(line, vocab):\n",
    "    # If line is empty return placeholders for word and tag\n",
    "    if not line.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "    else:\n",
    "        word, tag = line.split()\n",
    "#         Handling unknown words \n",
    "        if word not in vocab: \n",
    "            word = assign_unk(word)\n",
    "    return word, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--n--', '--s--')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('In', 'IN')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('In\\tIN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('--unk--', 'NN')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_tag('tardigrade\\tNN\\n', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, data_fp):\n",
    "    orig = []\n",
    "    prep = []\n",
    "\n",
    "    with open(data_fp, \"r\") as data_file:\n",
    "\n",
    "        for cnt, word in enumerate(data_file):\n",
    "#             cnt=0, word='The'\n",
    "#             word= '\\n'\n",
    "            if not word.split():\n",
    "                orig.append(word.strip())\n",
    "                word = \"--n--\"\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            # Handle unknown words\n",
    "# \n",
    "            elif word.strip() not in vocab:\n",
    "                orig.append(word.strip())\n",
    "                word = assign_unk(word)\n",
    "                prep.append(word)\n",
    "                continue\n",
    "\n",
    "            else:\n",
    " #             word='The'\n",
    "                orig.append(word.strip())\n",
    "                prep.append(word.strip())\n",
    "\n",
    "    assert(len(orig) == len(open(data_fp, \"r\").readlines()))\n",
    "    assert(len(prep) == len(open(data_fp, \"r\").readlines()))\n",
    "\n",
    "    return orig, prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "with open(\"WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "# load vocab\n",
    "with open(\"vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab: dictionary that has the index of the corresponding words\n",
    "vocab = {} \n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary dictionary, key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary dictionary, key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# load in the test corpus\n",
    "with open(\"WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()\n",
    "print(y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rebound', 'in', 'permits', 'for', 'multifamily', 'units', 'signaled', 'an', 'increase', 'in', 'September', 'starts', ',', 'though', 'activity', 'remains', 'fairly', 'modest', 'by', 'historical', 'standards', '.', '--n--', '--unk_punct--', 'Street', '--n--', 'If', 'the', '--unk_punct--', '--unk_punct--', 'law', \"'s\", 'fair', ',', 'Why', 'should', 'we', 'not', 'then', 'amend', 'the', '--unk--', 'To', 'require', 'that', 'all', 'employees', 'give', 'Similar', 'notice', 'before', 'they', 'quit', '?', '--n--', '--', '--unk_upper--', 'S.', '--unk_upper--', '.', '--n--', '--unk_upper--', '--unk_upper--', '--n--', 'When', 'research', 'projects', 'are', 'curtailed', 'due', 'to', 'government', 'funding', 'cuts', ',', 'are', 'we', '``', 'caught', 'with', 'our', 'grants', 'down', \"''\", '?', '--n--', '--', '--unk_punct--', 'Friedman', '.', '--n--', 'Assuming', 'the', 'stock', 'market', 'does', \"n't\", 'crash', 'again', 'and', 'completely', '--unk--', 'yuppies', 'and', 'trading', 'rooms', ',', 'American', 'television', 'audiences', 'in', 'a', 'few', 'months', 'may', 'be', 'seeing', 'Britain', \"'s\", 'concept', 'of', 'both', '.', '--n--', '``', 'Capital', 'City', \"''\", 'is', 'a', 'weekly', 'series', 'that', '--unk--', 'here', 'three', 'weeks', 'ago', 'amid', 'unprecedented', 'hype', 'by', 'its', 'producer', ',', '--unk_upper--', 'Television', '.', '--n--', 'The', 'early', 'episodes', 'make', 'you', 'long', 'for', 'a', '--unk--', 'of', 'the', 'crash', 'of', '1987', '.', '--n--', 'Let', \"'s\", 'make', 'that', '1929', ',', 'just', 'to', 'be', 'sure', '.', '--n--', 'According', 'to', 'the', 'program', \"'s\", 'publicity', 'prospectus', ',', '``', 'Capital', 'City', ',', \"''\", 'set', 'at', '--unk_upper--', '--unk_upper--', ',', 'a', '--unk--', '--unk_punct--', 'securities', 'firm']\n"
     ]
    }
   ],
   "source": [
    "#corpus without tags, preprocessed\n",
    "ori, prep = preprocess(vocab, \"test.words\")     \n",
    "print(prep[600:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rebound', 'in', 'permits', 'for', 'multifamily', 'units', 'signaled', 'an', 'increase', 'in', 'September', 'starts', ',', 'though', 'activity', 'remains', 'fairly', 'modest', 'by', 'historical', 'standards', '.', '', 'Two-Way', 'Street', '', 'If', 'the', 'sixty-day', 'plant-closing', 'law', \"'s\", 'fair', ',', 'Why', 'should', 'we', 'not', 'then', 'amend', 'the', 'writ', 'To', 'require', 'that', 'all', 'employees', 'give', 'Similar', 'notice', 'before', 'they', 'quit', '?', '', '--', 'Rollin', 'S.', 'Trexler', '.', '', 'Candid', 'Comment', '', 'When', 'research', 'projects', 'are', 'curtailed', 'due', 'to', 'government', 'funding', 'cuts', ',', 'are', 'we', '``', 'caught', 'with', 'our', 'grants', 'down', \"''\", '?', '', '--', 'C.E.', 'Friedman', '.', '', 'Assuming', 'the', 'stock', 'market', 'does', \"n't\", 'crash', 'again', 'and', 'completely', 'discredit', 'yuppies', 'and', 'trading', 'rooms', ',', 'American', 'television', 'audiences', 'in', 'a', 'few', 'months', 'may', 'be', 'seeing', 'Britain', \"'s\", 'concept', 'of', 'both', '.', '', '``', 'Capital', 'City', \"''\", 'is', 'a', 'weekly', 'series', 'that', 'premiered', 'here', 'three', 'weeks', 'ago', 'amid', 'unprecedented', 'hype', 'by', 'its', 'producer', ',', 'Thames', 'Television', '.', '', 'The', 'early', 'episodes', 'make', 'you', 'long', 'for', 'a', 'rerun', 'of', 'the', 'crash', 'of', '1987', '.', '', 'Let', \"'s\", 'make', 'that', '1929', ',', 'just', 'to', 'be', 'sure', '.', '', 'According', 'to', 'the', 'program', \"'s\", 'publicity', 'prospectus', ',', '``', 'Capital', 'City', ',', \"''\", 'set', 'at', 'Shane', 'Longman', ',', 'a', 'fictional', 'mid-sized', 'securities', 'firm']\n"
     ]
    }
   ],
   "source": [
    "print(ori[600:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab):\n",
    "    \n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    # Initialize \"prev_tag\" (previous tag) with the start state, denoted by '--s--'\n",
    "    prev_tag = '--s--' \n",
    "    i = 0 \n",
    "    \n",
    "    # Each item in the training corpus contains a word and its POS tag\n",
    "    # Go through each word and its tag in the training corpus\n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        # Increment the word_tag count\n",
    "        i += 1\n",
    "    \n",
    "        # get the word and tag using the get_word_tag helper function \n",
    "        word, tag = get_word_tag(word_tag,vocab) \n",
    "        \n",
    "        # Increment the transition count for the previous word and tag\n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        # Increment the emission count for the tag and word\n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        # Increment the tag count\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        # Set the previous tag to this tag (for the next iteration of the loop)\n",
    "        prev_tag = tag\n",
    "    \n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition_counts\n",
      "\n",
      "('--s--', 'IN'):5050\n",
      "('IN', 'DT'):32364\n",
      "('DT', 'NNP'):9044\n",
      "('NNP', 'CD'):1752\n",
      "('CD', 'NN'):7377\n",
      "('NN', 'IN'):32885\n"
     ]
    }
   ],
   "source": [
    "print(\"transition_counts\\n\")\n",
    "cnt = 0\n",
    "for k,v in transition_counts.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emmision_counts\n",
      "\n",
      "('IN', 'In'):1735\n",
      "('DT', 'an'):3142\n",
      "('NNP', 'Oct.'):317\n",
      "('CD', '19'):100\n",
      "('NN', 'review'):36\n",
      "('IN', 'of'):22925\n"
     ]
    }
   ],
   "source": [
    "print(\"emmision_counts\\n\")\n",
    "cnt = 0\n",
    "for k,v in emission_counts.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_counts\n",
      "\n",
      "IN:98554\n",
      "DT:81842\n",
      "NNP:91466\n",
      "CD:36568\n",
      "NN:132935\n",
      "``:7092\n"
     ]
    }
   ],
   "source": [
    "print(\"tag_counts\\n\")\n",
    "cnt = 0\n",
    "for k,v in tag_counts.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "# get all the POS states\n",
    "states = sorted(tag_counts.keys())\n",
    "print(len(states))\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition examples: \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n",
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n",
      "ambiguous word example: \n",
      "('RB', 'back') 304\n",
      "('VB', 'back') 20\n",
      "('RP', 'back') 84\n",
      "('JJ', 'back') 25\n",
      "('NN', 'back') 29\n",
      "('VBP', 'back') 4\n"
     ]
    }
   ],
   "source": [
    "print(\"transition examples: \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "\n",
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()\n",
    "\n",
    "print(\"ambiguous word example: \")\n",
    "for tup,cnt in emission_counts.items():\n",
    "    if tup[1] == 'back': print (tup, cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# print the test set actual data\n",
    "print(y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n"
     ]
    }
   ],
   "source": [
    "# The test set is preprocessed to get only words not tag\n",
    "print(prep[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34199 34199\n"
     ]
    }
   ],
   "source": [
    "print(len(y), len(prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    num_correct = 0\n",
    "    \n",
    "    # Get the (tag, word) tuples, stored as a set\n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    # Get the number of (word, POS) tuples in the corpus 'y'\n",
    "    total = len(y)\n",
    "    \n",
    "# pre--> 'The'\n",
    "# Get the true label of the word from the test set (y)--> 'The\\tDT\\n'\n",
    "# zip--> ('The', 'The\\tDT\\n')\n",
    "# word= 'The' and y_tup= 'The\\tDT\\n'\n",
    "# y_tup_l= ['The', 'DT']\n",
    "\n",
    "# Example2- word= '--unk--'\n",
    "# y='vantage\\tNN\\n'\n",
    "#  zip(prep, y)= ('--unk--', 'vantage\\tNN\\n')\n",
    "# y_tup_l= ['vantage', 'NN']\n",
    "# true_label= 'NN'\n",
    "# word= '--unk--'\n",
    "    for word, y_tup in zip(prep, y): \n",
    "        y_tup_l = y_tup.split()\n",
    "        if len(y_tup_l) == 2:\n",
    "            true_label = y_tup_l[1]\n",
    "        else:\n",
    "            # If the y_tup didn't contain word and POS, go to next word\n",
    "            continue\n",
    "    \n",
    "       \n",
    "        \n",
    "# Example--> test set word = 'The'\n",
    "# check for all possible pos from states, if that (pos, word) exists in emmision count (from training \n",
    "# set)and save the max count in the count_final and get its tag, now compare this tag with the \n",
    "# true_label to calculate the accuracy\n",
    "\n",
    "#Example2--> if '--unk--' in vocab (yes)\n",
    "# states= ['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
    "# key1= ('#', --unk--')\n",
    "# key2= ('$', '--unk--')\n",
    "# ..\n",
    "# ..\n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "                key = (pos,word)\n",
    "                if key in emission_counts: \n",
    "                    count = emission_counts[key]\n",
    "\n",
    "                    if count>count_final:\n",
    "                        count_final = count\n",
    "                        pos_final = pos\n",
    "\n",
    "            if pos_final == true_label:\n",
    "                num_correct += 1\n",
    "    \n",
    "    accuracy = num_correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8889\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag_counts\n",
      "\n",
      "IN:98554\n",
      "DT:81842\n",
      "NNP:91466\n",
      "CD:36568\n",
      "NN:132935\n",
      "``:7092\n"
     ]
    }
   ],
   "source": [
    "print(\"tag_counts\\n\")\n",
    "cnt = 0\n",
    "for k,v in tag_counts.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "142\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "all_tags = sorted(tag_counts.keys()) \n",
    "print(all_tags)\n",
    "# print(len(all_tags))\n",
    "# count_prev_tag = tag_counts[all_tags[i]]\n",
    "count_prev_tag = tag_counts['#']\n",
    "print(count_prev_tag)\n",
    "print(transition_counts[('#', '#')])\n",
    "print(transition_counts[('#', '$')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transition count dictionary values \n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"transition count dictionary values \")\n",
    "for ex in list(transition_counts.items())[:3]:\n",
    "    print(ex)\n",
    "print()\n",
    "trans_keys = set(transition_counts.keys())\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    " \n",
    "    all_tags = sorted(tag_counts.keys()) \n",
    "    num_tags = len(all_tags)\n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "            if transition_counts: \n",
    "                count = transition_counts[key]\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A at row 0, col 0: 0.000007040\n",
      "A at row 3, col 1: 0.1691\n",
      "View a subset of transition matrix A\n",
      "              RBS            RP           SYM        TO            UH\n",
      "RBS  2.217069e-06  2.217069e-06  2.217069e-06  0.008870  2.217069e-06\n",
      "RP   3.756509e-07  7.516775e-04  3.756509e-07  0.051089  3.756509e-07\n",
      "SYM  1.722772e-05  1.722772e-05  1.722772e-05  0.000017  1.722772e-05\n",
      "TO   4.477336e-05  4.472863e-08  4.472863e-08  0.000090  4.477336e-05\n",
      "UH   1.030439e-05  1.030439e-05  1.030439e-05  0.061837  3.092348e-02\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "# Testing your function\n",
    "print(f\"A at row 0, col 0: {A[0,0]:.9f}\")\n",
    "print(f\"A at row 3, col 1: {A[3,1]:.4f}\")\n",
    "\n",
    "print(\"View a subset of transition matrix A\")\n",
    "A_sub = pd.DataFrame(A[30:35,30:35], index=states[30:35], columns = states[30:35] )\n",
    "print(A_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission examples: \n",
      "(('DT', 'any'), 721)\n",
      "(('NN', 'decrease'), 7)\n",
      "(('NN', 'insider-trading'), 5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"emission examples: \")\n",
    "for ex in list(emission_counts.items())[200:203]:\n",
    "    print (ex)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23777\n",
      "vocab is a dictionary where key is the word, value is a unique integer\n",
      ":0\n",
      "!:1\n",
      "#:2\n",
      "$:3\n",
      "%:4\n",
      "&:5\n",
      "['', '!', '#', '$', '%']\n"
     ]
    }
   ],
   "source": [
    "num_words = len(vocab)\n",
    "print(num_words)\n",
    "# print(type(vocab))\n",
    "# print(vocab)\n",
    "print(\"vocab is a dictionary where key is the word, value is a unique integer\")\n",
    "cnt = 0\n",
    "for k,v in vocab.items():\n",
    "    print(f\"{k}:{v}\")\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "        break\n",
    "print(list(vocab)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    " \n",
    "    num_tags = len(all_tags)\n",
    "    all_tags = sorted(tag_counts.keys()) \n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "#     trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "            key = (all_tags[i],all_tags[j])\n",
    "            if key in transition_counts.keys(): \n",
    "                count = transition_counts[key]\n",
    "            count_prev_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + alpha*num_tags)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    \n",
    "    num_tags = len(tag_counts)\n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    num_words = len(vocab)\n",
    "    B = np.zeros((num_tags, num_words))\n",
    "\n",
    "#     emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        for j in range(num_words):\n",
    "            count = 0\n",
    "            key =  (all_tags[i],vocab[j])\n",
    "            if key in emission_counts.keys():\n",
    "                count = emission_counts[key]\n",
    "            count_tag = tag_counts[all_tags[i]]\n",
    "            \n",
    "            B[i,j] = (count + alpha) / (count_tag+ alpha*num_words)\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View Matrix position at row 0, column 0: 0.000006032\n",
      "View Matrix position at row 3, column 1: 0.000000720\n",
      "              725      adroitly     engineers      promoted       synergy\n",
      "CD   8.201296e-05  2.732854e-08  2.732854e-08  2.732854e-08  2.732854e-08\n",
      "NN   7.521128e-09  7.521128e-09  7.521128e-09  7.521128e-09  2.257091e-05\n",
      "NNS  1.670013e-08  1.670013e-08  4.676203e-04  1.670013e-08  1.670013e-08\n",
      "VB   3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08  3.779036e-08\n",
      "RB   3.226454e-08  6.456135e-05  3.226454e-08  3.226454e-08  3.226454e-08\n",
      "RP   3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07  3.723317e-07\n"
     ]
    }
   ],
   "source": [
    "# creating your emission probability matrix. this takes a few minutes to run. \n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))\n",
    "\n",
    "print(f\"View Matrix position at row 0, column 0: {B[0,0]:.9f}\")\n",
    "print(f\"View Matrix position at row 3, column 1: {B[3,1]:.9f}\")\n",
    "\n",
    "# Try viewing emissions for a few words in a sample dataframe\n",
    "cidx  = ['725','adroitly','engineers', 'promoted', 'synergy']\n",
    "\n",
    "# Get the integer ID for each word\n",
    "cols = [vocab[a] for a in cidx]\n",
    "\n",
    "# Choose POS tags to show in a sample dataframe\n",
    "rvals =['CD','NN','NNS', 'VB','RB','RP']\n",
    "\n",
    "# For each POS tag, get the row number from the 'states' list\n",
    "rows = [states.index(a) for a in rvals]\n",
    "\n",
    "# Get the emissions for the sample of words, and the sample of POS tags\n",
    "B_sub = pd.DataFrame(B[np.ix_(rows,cols)], index=rvals, columns = cidx )\n",
    "print(B_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.03219988e-06 6.03219988e-06 8.56578416e-01 ... 6.03219988e-06\n",
      "  6.03219988e-06 6.03219988e-06]\n",
      " [1.35212298e-07 1.35212298e-07 1.35212298e-07 ... 1.35212298e-07\n",
      "  1.35212298e-07 1.35212298e-07]\n",
      " [1.44034584e-07 1.44034584e-07 1.44034584e-07 ... 1.44034584e-07\n",
      "  1.44034584e-07 1.44034584e-07]\n",
      " ...\n",
      " [5.21438963e-06 5.21438963e-06 5.21438963e-06 ... 5.21438963e-06\n",
      "  5.21438963e-06 5.21438963e-06]\n",
      " [4.61514960e-07 4.61514960e-07 4.61514960e-07 ... 4.61514960e-07\n",
      "  4.61514960e-07 4.61514960e-07]\n",
      " [1.40532791e-07 1.40532791e-07 1.40532791e-07 ... 1.40532791e-07\n",
      "  1.40532791e-07 1.40532791e-07]]\n"
     ]
    }
   ],
   "source": [
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34199"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n",
      "The\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(prep[:10])\n",
    "print(prep[0])\n",
    "vocab['The']\n",
    "s_idx = states.index(\"--s--\")\n",
    "print(states)\n",
    "print(s_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.60982633354825\n",
      "-4.016237413730867\n"
     ]
    }
   ],
   "source": [
    "print(math.log(A[6,0])+math.log(B[0,8614]))\n",
    "print(math.log(A[6,11])+math.log(B[11,8614]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8614\n",
      "6.032199882975323e-06\n",
      "0.0830017285489149\n"
     ]
    }
   ],
   "source": [
    "print(vocab['The'])\n",
    "print(B[0,8614])\n",
    "print(B[11,8614])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        states: a list of all possible parts-of-speech\n",
    "        tag_counts: a dictionary mapping each tag to its respective count\n",
    "        A: Transition Matrix of dimension (num_tags, num_tags)\n",
    "        B: Emission Matrix of dimension (num_tags, len(vocab))\n",
    "        corpus: a sequence of words whose POS is to be identified in a list \n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index\n",
    "    Output:\n",
    "        best_probs: matrix of dimension (num_tags, len(corpus)) of floats\n",
    "        best_paths: matrix of dimension (num_tags, len(corpus)) of integers\n",
    "    '''\n",
    "   \n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    s_idx = states.index(\"--s--\")\n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        if A[s_idx,i] == 0: \n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0, to negative infinity\n",
    "            best_probs[i,0] = float('-inf')\n",
    "        \n",
    "        # For all other cases when transition from start token to POS tag i is non-zero:\n",
    "        else:\n",
    "            \n",
    "            # Initialize best_probs at POS tag 'i', column 0\n",
    "            # Check the formula in the instructions above\n",
    "            best_probs[i,0] = math.log(A[s_idx,i]) + math.log(B[i,vocab[corpus[0]]] )\n",
    "                        \n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.03997297e-06 7.03997297e-06 7.03997297e-06 ... 7.03997297e-06\n",
      "  7.03997297e-06 7.03997297e-06]\n",
      " [1.35647553e-07 1.35647553e-07 1.35647553e-07 ... 1.35647553e-07\n",
      "  1.35647553e-07 1.35647553e-07]\n",
      " [1.44528595e-07 1.44673124e-04 6.93751711e-03 ... 2.89201719e-04\n",
      "  3.17977363e-03 2.74618784e-03]\n",
      " ...\n",
      " [5.95075158e-06 2.38089571e-02 5.95075158e-06 ... 5.95075158e-06\n",
      "  5.95075158e-06 5.95670233e-03]\n",
      " [4.66625541e-07 4.67092167e-04 4.66625541e-07 ... 4.66625541e-07\n",
      "  4.67092167e-04 1.86696879e-03]\n",
      " [1.41003034e-07 1.41144037e-04 1.41003034e-07 ... 1.41003034e-07\n",
      "  1.12803837e-02 4.23150104e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,0]: -22.6098\n",
      "best_probs[1,0]: -23.0766\n",
      "best_probs[2,0]: -23.5730\n",
      "best_probs[3,0]: -19.7673\n",
      "best_probs[4,0]: -24.7433\n",
      "best_probs[5,0]: -35.2024\n",
      "best_probs[6,0]: -35.0010\n",
      "best_probs[7,0]: -34.9920\n",
      "best_probs[8,0]: -21.3507\n",
      "best_probs[9,0]: -19.8577\n",
      "best_probs[10,0]: -21.9210\n",
      "best_probs[11,0]: -4.0162\n",
      "best_probs[12,0]: -19.1638\n",
      "best_probs[13,0]: -21.1062\n",
      "best_probs[14,0]: -20.4716\n",
      "best_probs[15,0]: -21.1016\n",
      "best_probs[16,0]: -21.4958\n",
      "best_probs[17,0]: -20.4812\n",
      "best_probs[18,0]: -18.2586\n",
      "best_probs[19,0]: -23.3972\n",
      "best_probs[20,0]: -21.9215\n",
      "best_probs[21,0]: -9.4138\n",
      "best_probs[22,0]: -21.0305\n",
      "best_probs[23,0]: -21.0803\n",
      "best_probs[24,0]: -20.1086\n",
      "best_probs[25,0]: -33.4819\n",
      "best_probs[26,0]: -19.4730\n",
      "best_probs[27,0]: -20.7715\n",
      "best_probs[28,0]: -20.1173\n",
      "best_probs[29,0]: -20.5603\n",
      "best_probs[30,0]: -20.5719\n",
      "best_probs[31,0]: -32.3037\n",
      "best_probs[32,0]: -18.0755\n",
      "best_probs[33,0]: -22.5889\n",
      "best_probs[34,0]: -19.1586\n",
      "best_probs[35,0]: -16.0299\n",
      "best_probs[36,0]: -24.3097\n",
      "best_probs[37,0]: -20.9293\n",
      "best_probs[38,0]: -21.9680\n",
      "best_probs[39,0]: -24.2957\n",
      "best_probs[40,0]: -23.4597\n",
      "best_probs[41,0]: -22.4367\n",
      "best_probs[42,0]: -20.4657\n",
      "best_probs[43,0]: -22.7555\n",
      "best_probs[44,0]: -19.6637\n",
      "best_probs[45,0]: -18.3629\n"
     ]
    }
   ],
   "source": [
    "# print first column of the best_prob matrix \n",
    "for i in range(len(states)):\n",
    "    print(f\"best_probs[{i},0]: {best_probs[i,0]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--']\n",
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "len(prep)\n",
    "print(prep[:10])\n",
    "print(states)\n",
    "print(states.index('NN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_forward\n",
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab):\n",
    "    '''\n",
    "    Input: \n",
    "        A, B: The transiton and emission matrices respectively\n",
    "        test_corpus: a list containing a preprocessed corpus\n",
    "        best_probs: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: an initilized matrix of dimension (num_tags, len(corpus))\n",
    "        vocab: a dictionary where keys are words in vocabulary and value is an index \n",
    "    Output: \n",
    "        best_probs: a completed matrix of dimension (num_tags, len(corpus))\n",
    "        best_paths: a completed matrix of dimension (num_tags, len(corpus))\n",
    "    '''\n",
    "    # Get the number of unique POS tags (which is the num of rows in best_probs)\n",
    "    num_tags = best_probs.shape[0]\n",
    " \n",
    "    for i in range(1, len(test_corpus)): \n",
    "        for j in range(num_tags):\n",
    "            \n",
    "            best_prob_i = float(\"-inf\")\n",
    "            best_path_i = None\n",
    "\n",
    "            for k in range(num_tags):\n",
    "                prob = best_probs[k,i-1]+math.log(A[k,j]) +math.log(B[j,vocab[test_corpus[i]]])\n",
    "                \n",
    "                if prob > best_prob_i:\n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "                    \n",
    "            best_probs[j,i] = best_prob_i\n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a few minutes to run => processes ~ 30,000 words\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'economy'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_probs[0,1]: -24.7822\n",
      "best_probs[1,1]: -24.5158\n",
      "best_probs[2,1]: -29.9831\n",
      "best_probs[3,1]: -25.7122\n",
      "best_probs[4,1]: -28.7870\n",
      "best_probs[5,1]: -27.7787\n",
      "best_probs[6,1]: -30.8835\n",
      "best_probs[7,1]: -27.9454\n",
      "best_probs[8,1]: -27.2155\n",
      "best_probs[9,1]: -28.2268\n",
      "best_probs[10,1]: -25.2072\n",
      "best_probs[11,1]: -28.6896\n",
      "best_probs[12,1]: -33.8392\n",
      "best_probs[13,1]: -24.7441\n",
      "best_probs[14,1]: -27.0618\n",
      "best_probs[15,1]: -23.4693\n",
      "best_probs[16,1]: -24.1845\n",
      "best_probs[17,1]: -23.1868\n",
      "best_probs[18,1]: -33.2349\n",
      "best_probs[19,1]: -26.2309\n",
      "best_probs[20,1]: -10.9601\n",
      "best_probs[21,1]: -24.5507\n",
      "best_probs[22,1]: -24.1057\n",
      "best_probs[23,1]: -24.5391\n",
      "best_probs[24,1]: -28.2113\n",
      "best_probs[25,1]: -28.2834\n",
      "best_probs[26,1]: -28.3153\n",
      "best_probs[27,1]: -27.1986\n",
      "best_probs[28,1]: -25.8399\n",
      "best_probs[29,1]: -24.7787\n",
      "best_probs[30,1]: -22.9700\n",
      "best_probs[31,1]: -28.3403\n",
      "best_probs[32,1]: -32.1483\n",
      "best_probs[33,1]: -29.0336\n",
      "best_probs[34,1]: -26.3368\n",
      "best_probs[35,1]: -29.3754\n",
      "best_probs[36,1]: -27.3496\n",
      "best_probs[37,1]: -25.4011\n",
      "best_probs[38,1]: -25.6072\n",
      "best_probs[39,1]: -26.6945\n",
      "best_probs[40,1]: -25.7337\n",
      "best_probs[41,1]: -27.8344\n",
      "best_probs[42,1]: -25.8246\n",
      "best_probs[43,1]: -34.4006\n",
      "best_probs[44,1]: -29.9165\n",
      "best_probs[45,1]: -24.9885\n"
     ]
    }
   ],
   "source": [
    "# Test this function --> check for the word 'economy'\n",
    "for i in range(len(states)):\n",
    "    print(f\"best_probs[{i},1]: {best_probs[i,1]:.4f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: viterbi_backward\n",
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    '''\n",
    "    This function returns the best path.\n",
    "    \n",
    "    '''\n",
    "    # Get the number of words in the corpus\n",
    "    # which is also the number of columns in best_probs, best_paths\n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    # Initialize array z, same length as the corpus\n",
    "    z = [None] * m\n",
    "    \n",
    "    # Get the number of unique POS tags\n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    # Initialize the best probability for the last word\n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    # Initialize pred array, same length as corpus\n",
    "    pred = [None] * m\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    ## Step 1 ##\n",
    "    \n",
    "    # Go through each POS tag for the last word (last column of best_probs)\n",
    "    # in order to find the row (POS tag integer ID) \n",
    "    # with highest probability for the last word\n",
    "    for k in range(num_tags): # complete this line\n",
    "\n",
    "        # If the probability of POS tag at row k \n",
    "        # is better than the previosly best probability for the last word:\n",
    "        if best_probs[k,-1]>best_prob_for_last_word: # complete this line\n",
    "            \n",
    "            # Store the new best probability for the lsat word\n",
    "            best_prob_for_last_word = best_probs[k,-1]\n",
    "    \n",
    "            # Store the unique integer ID of the POS tag\n",
    "            # which is also the row number in best_probs\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    # Convert the last word's predicted POS tag\n",
    "    # from its unique integer ID into the string representation\n",
    "    # using the 'states' dictionary\n",
    "    # store this in the 'pred' array for the last word\n",
    "    pred[m - 1] = states[k]\n",
    "    \n",
    "    ## Step 2 ##\n",
    "    # Find the best POS tags by walking backward through the best_paths\n",
    "    # From the last word in the corpus to the 0th word in the corpus\n",
    "    for i in range(len(corpus)-1, -1, -1): # complete this line\n",
    "        \n",
    "        # Retrieve the unique integer ID of\n",
    "        # the POS tag for the word at position 'i' in the corpus\n",
    "        pos_tag_for_word_i = best_paths[np.argmax(best_probs[:,i]),i]\n",
    "        \n",
    "        # In best_paths, go to the row representing the POS tag of word i\n",
    "        # and the column representing the word's position in the corpus\n",
    "        # to retrieve the predicted POS for the word at position i-1 in the corpus\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i,i]\n",
    "        \n",
    "        # Get the previous word's POS tag in string form\n",
    "        # Use the 'states' dictionary, \n",
    "        # where the key is the unique integer ID of the POS tag,\n",
    "        # and the value is the string representation of that POS tag\n",
    "        pred[i - 1] = states[pos_tag_for_word_i]\n",
    "        \n",
    "     ### END CODE HERE ###\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for pred[-7:m-1] is: \n",
      " ['see', 'them', 'here', 'with', 'us', '.'] \n",
      " ['VB', 'PRP', 'RB', 'IN', 'PRP', '.'] \n",
      "\n",
      "The prediction for pred[0:8] is: \n",
      " ['DT', 'NN', 'POS', 'NN', 'MD', 'VB', 'VBN'] \n",
      " ['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken']\n"
     ]
    }
   ],
   "source": [
    "# Run and test your function\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)\n",
    "m=len(pred)\n",
    "print('The prediction for pred[-7:m-1] is: \\n', prep[-7:m-1], \"\\n\", pred[-7:m-1], \"\\n\")\n",
    "print('The prediction for pred[0:8] is: \\n', pred[0:7], \"\\n\", prep[0:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third word is: temperature\n",
      "Your prediction is: NN\n",
      "Your corresponding label y is:  temperature\tNN\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('The third word is:', prep[3])\n",
    "print('Your prediction is:', pred[3])\n",
    "print('Your corresponding label y is: ', y[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(pred, y):\n",
    "    '''\n",
    "    Input: \n",
    "        pred: a list of the predicted parts-of-speech \n",
    "        y: a list of lines where each word is separated by a '\\t' (i.e. word \\t tag)\n",
    "    Output: \n",
    "        \n",
    "    '''\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Zip together the prediction and the labels\n",
    "    for prediction, y in zip(pred, y):\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # Split the label into the word and the POS tag\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        # Check that there is actually a word and a tag\n",
    "        # no more and no less than 2 items\n",
    "        if len(word_tag_tuple)!=2: # complete this line\n",
    "            continue \n",
    "\n",
    "        # store the word and tag separately\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        # Check if the POS tag label matches the prediction\n",
    "        if prediction == tag: # complete this line\n",
    "            \n",
    "            # count the number of times that the prediction\n",
    "            # and label match\n",
    "            num_correct += 1\n",
    "            \n",
    "        # keep track of the total number of examples (that have valid labels)\n",
    "        total += 1\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9528\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
